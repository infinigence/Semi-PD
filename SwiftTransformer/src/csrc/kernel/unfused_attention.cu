#include "unfused_attention.h"

#include "util/cuda_utils.h"

namespace st::kernel {

/*
    transposeQKVKernel &
    transposeQKV

    This kernel retrieves q, k, and v in the particular input from QKV buffer
    (generated by previous step, qkv_gemm), and saves them
    to q_buf, k_buf, and v_buf respectively.

    The input buffer, QKV, has a shape of [num_tokens, 3, num_heads, head_dim]
    The output buffer, q_buf, k_buf, and v_buf, has a shape of [num_heads, cur_input_len, head_dim]

    The selected input (request) is indicated by cur_input_start.
*/
template<typename T>
__global__ void transposeQKVKernel(
	T* q_buf,
	T* k_buf,
	T* v_buf,
	const T* QKV,
	const int64_t cur_input_start,
	const int64_t input_len,
	const int64_t num_heads,
	const int64_t head_dim
) {
    const int64_t size_index = threadIdx.x;
    const int64_t head_index = blockIdx.x;
    for (int64_t token_index = 0; token_index < input_len; ++token_index) {
		int64_t qkv_q_index = INDEX_4D(input_len, 3, num_heads, head_dim, token_index+cur_input_start, 0, head_index, size_index);
		int64_t qkv_k_index = qkv_q_index + num_heads*head_dim;
		int64_t qkv_v_index = qkv_k_index + num_heads*head_dim;
		int64_t q_buf_index = INDEX_3D(num_heads, input_len, head_dim, head_index, token_index, size_index);
		int64_t k_buf_index = q_buf_index;
		int64_t v_buf_index = q_buf_index;
		q_buf[q_buf_index] = QKV[qkv_q_index];
		k_buf[k_buf_index] = QKV[qkv_k_index];
		v_buf[v_buf_index] = QKV[qkv_v_index];
    }
}

template<typename T>
void transposeQKV(
	T* q_buf,
	T* k_buf,
	T* v_buf,
	const T* QKV,
	const int64_t cur_input_start,
	const int64_t input_len,
	const int64_t num_heads,
	const int64_t head_dim
) {
    transposeQKVKernel<<<num_heads, head_dim>>>(
		q_buf,
		k_buf,
		v_buf,
		QKV,
		cur_input_start,
		input_len,
		num_heads,
		head_dim
	);
}

template void transposeQKV(
	float* q_buf, float* k_buf, float* v_buf,
	const float* QKV, 
	const int64_t cur_input_start, const int64_t input_len, const int64_t num_heads, const int64_t head_dim
);

template void transposeQKV(
	half* q_buf, half* k_buf, half* v_buf,
	const half* QKV,
	const int64_t cur_input_start, const int64_t input_len, const int64_t num_heads, const int64_t head_dim
);


/*
	mergeOutputKernel & mergeOutput

	This kernel is used in ContextDecoder to merge the output of the attention matrix into a single matrix.

	Input:
		- cur_input: [num_heads, cur_input_num_tokens, head_dim]
	Output:
		- output: [~, num_heads, head_dim,]
		  output[cur_input_start + i][head_index][size_index] = input[head_index][i][size_index]
*/
template<typename T>
__global__ void mergeOutputKernel(
	T* output,
	const T* cur_input,
	const int64_t cur_input_num_tokens,
	const int64_t cur_input_start,
	const int64_t num_heads,
	const int64_t head_dim
) {
    const int64_t size_index = threadIdx.x;
    const int64_t head_index = blockIdx.x;
    for (int64_t token_index = 0; token_index < cur_input_num_tokens; ++token_index) {
		int64_t output_index = INDEX_3D(0, num_heads, head_dim, cur_input_start+token_index, head_index, size_index);
		int64_t input_index = INDEX_3D(num_heads, cur_input_num_tokens, head_dim, head_index, token_index, size_index);
		output[output_index] = cur_input[input_index];
    }
}

template<typename T>
void mergeOutput(
	T* output,
	const T* cur_input,
	const int64_t cur_input_num_tokens,
	const int64_t cur_input_start,
	const int64_t num_heads,
	const int64_t head_dim,
	cudaStream_t stream
) {
    mergeOutputKernel<<<num_heads, head_dim, 0, stream>>>(output, cur_input, cur_input_num_tokens, cur_input_start, num_heads, head_dim);
}

template void mergeOutput(
	float* output, const float* cur_input,
	const int64_t cur_input_num_tokens, const int64_t cur_input_start, const int64_t num_heads, const int64_t head_dim, cudaStream_t stream
);
template void mergeOutput(
	half* output, const half* cur_input,
	const int64_t cur_input_num_tokens, const int64_t cur_input_start, const int64_t num_heads, const int64_t head_dim, cudaStream_t stream
);


}	// namespace st::kernel
